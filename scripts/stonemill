#!/usr/bin/env python3

import os
import sys
import re
import argparse


class Definition(object):
	def __init__(self, filepath, text, append=False, append_if_not_exist=False, ignore_missing=False):
		super(Definition, self).__init__()
		self.filepath = filepath
		self.text = text
		self.append = append
		self.append_if_not_exist = append_if_not_exist
		self.ignore_missing = ignore_missing
	def but_replace(self, key, replacement):
		return Definition(filepath=self.filepath,
				append=self.append,
				append_if_not_exist=self.append_if_not_exist,
				ignore_missing=self.ignore_missing,
				text=self.text.replace(key, replacement))
	def and_append(self, addendum):
		return Definition(filepath=self.filepath,
				append=self.append,
				append_if_not_exist=self.append_if_not_exist,
				ignore_missing=self.ignore_missing,
				text=self.text + addendum)
	def substitute_filepath(self, **args):
		t = self.filepath
		for k in args:
			t = t.replace('{{'+k+'}}', args[k])
		return t
	def substitute_text(self, **args):
		t = self.text
		for k in args:
			t = t.replace('{{'+k+'}}', args[k])
		return t
	def check_file(self, **args):
		filepath = self.substitute_filepath(**args)
		if self.append:
			if not self.ignore_missing and not os.path.exists(filepath):
				raise Exception('append file missing: ' + filepath)
		elif self.append_if_not_exist:
			pass
		else:
			if os.path.exists(filepath):
				raise Exception('write file already exists: ' + filepath)
	def write_file(self, **args):
		filepath = self.substitute_filepath(**args)
		text = self.substitute_text(**args)

		if self.append:
			print('[+] appending to', filepath)
			with open(filepath, 'a') as f:
				f.write(text)

		elif self.append_if_not_exist:
			if not os.path.exists(os.path.dirname(filepath)):
				print('[+] making directory', os.path.dirname(filepath))
				os.makedirs(os.path.dirname(filepath))
			if os.path.exists(filepath):
				with open(filepath, 'r') as f:
					existing_text = f.read()
			else:
				existing_text = ''
			if text not in existing_text:
				print('[+] appending to', filepath)
				with open(filepath, 'a') as f:
					f.write(text)
			else:
				print('[i] skipping', filepath, 'because it already contains the text:[[', text, ']] as you can see here: [[[[', existing_text, ']]]]')

		else:
			if not os.path.exists(os.path.dirname(filepath)):
				print('[+] making directory', os.path.dirname(filepath))
				os.makedirs(os.path.dirname(filepath))
			print('[+] writing to', filepath)
			with open(filepath, 'w') as f:
				f.write(text)

class TemplateDefinitions(object):
	def __init__(self, message, arguments, definitions, post_message, unformatted_post_message=None):
		super(TemplateDefinitions, self).__init__()
		self.message = message
		self.arguments = arguments
		self.definitions = definitions
		self.post_message = post_message
		self.unformatted_post_message = unformatted_post_message

	def parse_arguments(self, **template_args):
		parsed_args = {}
		for k in self.arguments:
			if template_args.get(k) is None:
				raise Exception('error: missing argument: ' + k)
			if not re.match(self.arguments[k], template_args[k]):
				raise Exception('error: invalid ' + k + ' arugment: ' + template_args[k] + ' (doesnt match required regex: /' + self.arguments[k] + '/)')
			parsed_args[k] = template_args[k]
		return parsed_args
				
	def mill_template(self, template_args):
		name = self.message.format(**template_args)
		print('[[preping {}...]]'.format(name))
		for d in self.definitions:
			d.check_file(**template_args)
		print('[[milling {}...]]'.format(name))
		for d in self.definitions:
			d.write_file(**template_args)
		print('[[done {}!]]'.format(name))
		print(self.post_message.format(**template_args))
		if self.unformatted_post_message is not None:
			print(self.unformatted_post_message)


base_terraform_main = Definition("infrastructure/main.tf", text='''provider "aws" {
  region = "us-east-1"

  default_tags {
	tags = {
	  Name = "${local.infragroup_fullname}"
	}
  }
}

variable "company_name" { default = "{{company_name}}" }
variable "group_name" { default = "{{infra_name}}" }
variable "stage" { default = "beta" }

locals {
  metrics_path = "${var.company_name}/${local.infragroup_fullname}"
  infragroup_fullname = "${var.group_name}-${var.stage}"
}

# resource group to track active infrastructure resources
resource "aws_resourcegroups_group" "infra_resource_group" {
name = "${local.infragroup_fullname}-resourcegroup"

  resource_query {
	query = <<JSON
{
  "ResourceTypeFilters": ["AWS::AllSupported"],
  "TagFilters": [
	{
	  "Key": "Name",
	  "Values": ["${local.infragroup_fullname}"]
	}
  ]
}
JSON
  }
}

# dashboard for monitoring the overall system. add widgets as necessary
resource "aws_cloudwatch_dashboard" "service_monitoring_dashboard" {
  dashboard_name = "${local.infragroup_fullname}-dashboard"
  dashboard_body = <<EOF
{
  "widgets": []
}
EOF
}
''')

base_makefile = Definition("./Makefile", text='''
include .env
export

configure:
	@aws configure set aws_access_key_id ${AWS_ACCESS_ID}
	@aws configure set aws_secret_access_key ${AWS_ACCESS_KEY}
	./scripts/aws-authenticate-mfa.sh

bash: configure
	bash

deploy:
	./scripts/deploy.sh -deploy

destroy:
	./scripts/deploy.sh -destroy

clean:
	rm -rf build .keys infrastructure/.terraform */modules */node_modules */dist infrastructure/terraform.tfstate*

build:

''')

base_dockerfile = Definition("./docker/Dockerfile", text='''#!/usr/bin/env -S bash -c "docker run -v \${PWD}:/app --rm --cap-drop=all -it \$(docker build -q docker) \$@"
FROM ubuntu:22.04

WORKDIR /app

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y gnupg software-properties-common curl \\
	&& curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add - \\
	&& apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main" \\
	&& apt-get update \\
	&& apt-get install -y nano jq zip make \\
	&& apt-get install -y terraform \\
	&& apt-get install -y python3-venv \\
	&& apt-get install -y openssh-client \\
	&& apt autoclean \\
	&& apt autoremove \\
	&& apt clean \\
	&& curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" \\
	&& unzip awscliv2.zip \\
	&& ./aws/install \\
	&& rm -rf awscliv2.zip aws

RUN ln -s /app/scripts/weasel /usr/bin/weasel
RUN ln -s /app/scripts/stonemill /usr/bin/stonemill

# safety measure to drop privileges to lowly user
RUN useradd -ms /bin/bash runuser
USER runuser

CMD make bash


''')

base_lambda_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{lambda_name}}_build_path" { default = "../build/{{lambda_name}}.zip" }

module "{{lambda_name}}" {
  source = "./{{lambda_name}}"

  metrics_path = local.metrics_path
  infragroup_fullname = local.infragroup_fullname
  lambda_build_path = var.{{lambda_name}}_build_path
}
''')

base_lambda_makefile = Definition("./Makefile", append=True, text='''
build_{{lambda_name}}:
	-mkdir build
	cd {{lambda_name}} && bash ./build.sh
''')

base_lambda_definition = Definition("infrastructure/{{lambda_name}}/{{lambda_name}}.tf", text='''
variable "metrics_path" { type = string }
variable "infragroup_fullname" { type = string }
variable "lambda_build_path" { type = string }

locals {
  fullname = "${var.infragroup_fullname}-{{lambda_name}}"
  metrics_group = "${var.metrics_path}/{{lambda_name}}"
}

resource "aws_lambda_function" "lambda_function" {
  function_name = "${local.fullname}"
  publish = true

  runtime = "python3.8"
  handler = "main.lambda_handler"

  filename = var.lambda_build_path
  source_code_hash = filebase64sha256(var.lambda_build_path)

  role = aws_iam_role.lambda_execution_role.arn

  memory_size = 512
  timeout = 30

  environment {
	variables = {
	  INFRAGROUP_FULLNAME = var.infragroup_fullname
	  METRICS_GROUP = local.metrics_group
	}
  }
}

resource "aws_lambda_function_event_invoke_config" "lambda_function_invoke_config" {
  function_name = aws_lambda_function.lambda_function.function_name
  maximum_retry_attempts = 0
}

resource "aws_cloudwatch_log_group" "lambda_function_cloudwatch" {
  name = "/aws/lambda/${aws_lambda_function.lambda_function.function_name}"
  retention_in_days = 90
}

resource "aws_iam_role" "lambda_execution_role" {
  name = "${local.fullname}-role"

  assume_role_policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  Action = "sts:AssumeRole"
	  Effect = "Allow"
	  Sid    = ""
	  Principal = {
		Service = "lambda.amazonaws.com"
	  }
	}]
  })
}

resource "aws_iam_policy" "lambda_policy" {
  name        = "${local.fullname}-policy"
  path        = "/"

  policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  Effect = "Allow"
	  Action = "logs:CreateLogGroup"
	  Resource = "*"
	}, {
	  Effect = "Allow"
	  Action = [ "logs:CreateLogStream", "logs:PutLogEvents" ]
	  Resource = "*"
	}, {
	  Effect = "Allow"
	  Action = "cloudwatch:PutMetricData"
	  Resource = "*"
	  Condition = {
		StringEquals = {
		  "cloudwatch:namespace" = local.metrics_group
		}
	  }
	}]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_policy_attachment" {
  role       = aws_iam_role.lambda_execution_role.name
  policy_arn = aws_iam_policy.lambda_policy.arn
}

# # crash alarm, will alarm when the lambda crashes recently
# resource "aws_cloudwatch_metric_alarm" "crash_alarm" {
#   alarm_name = "${local.fullname}-crash_alarm"
#   comparison_operator = "GreaterThanOrEqualToThreshold"
#   evaluation_periods = "1"
#   metric_name = "lambda-crash"
#   namespace = local.metrics_group
#   period = "300"
#   statistic = "Sum"
#   threshold = "1"
#   alarm_actions = [var.sns_alarm_topic_arn]
# }

output "monitor_widget_json" {
  value = <<EOF
  {
	"height": 6,
	"width": 6,
	"type": "metric",
	"properties": {
	  "metrics": [
		[ "${local.metrics_group}", "lambda-success", { "color": "#98df8a", "label": "lambda-success" } ],
		[ ".", "lambda-error", { "color": "#ffbb78" } ],
		[ ".", "lambda-crash", { "color": "#ff9896" } ]
	  ],
	  "view": "timeSeries",
	  "stacked": true,
	  "region": "us-east-1",
	  "stat": "Sum",
	  "period": 300,
	  "title": "{{lambda_name}} metrics"
	}
  }
EOF
}

''')

base_lambda_main = Definition("{{lambda_name}}/main.py", text='''import sys
sys.path.append('modules')
import os
import json
import boto3
import traceback

import lambda_routing
import routes



# aws api clients
cloudwatch_client = boto3.client('cloudwatch')
# env variables passed in from infrastructure
metrics_group = os.environ.get('METRICS_GROUP')



def lambda_handler(event, context):
	print('[i] handling event:', event)
	try:
		# perform routing
		res = route_request(event)
		print('[r] response:', res)
		if res.get('success') is not None and res['success']:
			send_metric('lambda-success')
		else:
			send_metric('lambda-error')
		return res
	except Exception as e:
		print('[!] exception thrown:', e, traceback.format_exception(None, e, e.__traceback__))
		# track crash metrics
		send_metric('lambda-crash')
		res = { 'success': False, 'error': 'server-side error' }
		print('[r] response:', res)
		return res

def send_metric(name):
	cloudwatch_client.put_metric_data(Namespace=metrics_group,
		MetricData = [{
			'MetricName': name,
			'Unit': 'Count',
			'Value': 1
		}])

def route_sqs_request(event):
	results = []
	for record in event['Records']:
		results.append(route_request(record))
	return results

def route_request(event):
	# get the json data from the body
	try:
		data = json.loads(str(event['body']))
	except ValueError as e:
		print('[!] error parsing json body:', e)
		return { 'success': False, 'error': 'invalid request' }

	# parse the action
	if data.get('action') is None:
		print('[!] missing action')
		return { 'success': False, 'error': 'invalid request' }
	action = str(data['action'])

	# try to route it
	route_fun = lambda_routing.get_route(action)
	if route_fun is None:
		print('[!] invalid action: ', action)
		return { 'success': False, 'error': 'invalid request' }
	
	# handle the routing
	print('[i] executing route: ' + action)
	return route_fun(data)
''')

base_lambda_lib = Definition("{{lambda_name}}/lambda_routing.py", text='''all_lambda_routes = {}
def lambda_route(route):
	def lambda_route_lambda_route(f):
		def wrapper(*args, **kwargs):
			return f(*args, **kwargs)
		all_lambda_routes[route] = wrapper
		return wrapper
	return lambda_route_lambda_route

def authenticated_lambda_route(route, authentication_fun):
	def lambda_route_lambda_route(f):
		def wrapper(data, *args, **kwargs):
			account = authentication_fun(data)
			if account is None:
				return { 'success': False, 'error': 'unauthenticated' }
			else:
				return f(data, account, *args, **kwargs)
		all_lambda_routes[route] = wrapper
		return wrapper
	return lambda_route_lambda_route

def get_route(route):
	return all_lambda_routes.get(route)
''')

base_lambda_routes = Definition("{{lambda_name}}/routes.py", text='''from lambda_routing import lambda_route, authenticated_lambda_route

@lambda_route('/{{lambda_name}}/hello')
def hello(data):
	if data.get('msg') is None:
		print('[-] missing message')
		return { 'success': True, 'msg': 'hello world!' }
	else:
		return { 'success': True, 'msg': data['msg'] }
''')

base_lambda_build = Definition("{{lambda_name}}/build.sh", text='''#!/bin/bash
python3 -m venv my_venv
source my_venv/bin/activate
pip install --upgrade --ignore-installed -r requirements.txt
deactivate

rm -rf modules __pycache__
mkdir modules
# ls -la my_venv/lib/*/site-packages/
rm -rf my_venv/lib/*/site-packages/*.dist-info \\
	my_venv/lib/*/site-packages/setuptools \\
	my_venv/lib/*/site-packages/pip \\
	my_venv/lib/*/site-packages/botocore \\
	my_venv/lib/*/site-packages/*/__pycache__
cp -rf my_venv/lib/*/site-packages/* modules
rm -rf my_venv/

zip -r ../build/{{lambda_name}}.zip . -x build.sh -x "*__pycache__*"
''')

base_lambda_requirements = Definition("{{lambda_name}}/requirements.txt", text='''
pyjwt
''')

sqs_lambda_main = base_lambda_main.but_replace('''res = route_request(event)
		print('[r] response:', res)
		if res.get('success') is not None and res['success']:
			send_metric('lambda-success')
		else:
			send_metric('lambda-error')
		return res''', '''results = route_sqs_request(event)
		print('[r] response:', results)
		# track metrics
		for res in results:
			if res.get('success') is not None and res['success']:
				send_metric('lambda-success')
			else:
				send_metric('lambda-error')

		return results''')
sqs_lambda_module = base_lambda_module.and_append('''
output "{{lambda_name}}_sqs_queue_url" { value = module.{{lambda_name}}.sqs_queue_url }
''')
sqs_lambda_definition = base_lambda_definition.but_replace('''{
	  Effect = "Allow"
	  Action = "cloudwatch:PutMetricData"
	  Resource = "*"
	  Condition = {
		StringEquals = {
		  "cloudwatch:namespace" = local.metrics_group
		}
	  }
	}]''','''{
	  Effect = "Allow"
	  Action = "cloudwatch:PutMetricData"
	  Resource = "*"
	  Condition = {
		StringEquals = {
		  "cloudwatch:namespace" = local.metrics_group
		}
	  }
	}, {
	  Effect = "Allow",
	  Action = [
		  "sqs:ReceiveMessage",
		  "sqs:DeleteMessage",
		  "sqs:GetQueueAttributes"
	  ],
	  Resource = aws_sqs_queue.input_queue.arn
	}]''').and_append('''
resource "aws_sqs_queue" "input_queue" {
  name        = "${local.fullname}-input_queue"
  delay_seconds             = 90
  max_message_size          = 2048
  message_retention_seconds = 86400
  receive_wait_time_seconds = 10
  redrive_policy = jsonencode({
	deadLetterTargetArn = aws_sqs_queue.input_dlq.arn
	maxReceiveCount     = 4
  })
  visibility_timeout_seconds = 300
}

resource "aws_sqs_queue" "input_dlq" {
  name        = "${local.fullname}-input_dlq"
}

resource "aws_lambda_event_source_mapping" "event_source_mapping" {
  event_source_arn = aws_sqs_queue.input_queue.arn
  function_name    = aws_lambda_function.lambda_function.arn
  batch_size = 1
}


output "sqs_queue_arn" { value = aws_sqs_queue.input_queue.arn }
output "sqs_queue_url" { value = aws_sqs_queue.input_queue.url }
''')
sqs_lambda_test = Definition("scripts/test.sh", append=True, text='''
SQS_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{lambda_name}}_sqs_queue_url.value')
aws sqs send-message --region us-east-1 --queue-url "$SQS_URL" --message-body "{\"action\":\"/{{lambda_name}}/hello\"}"
''')

api_lambda_module = base_lambda_module.and_append('''
output "{{lambda_name}}_api_url" { value = module.{{lambda_name}}.api_url }
''')
api_lambda_definition = base_lambda_definition.and_append('''
resource "aws_apigatewayv2_api" "gateway_api" {
  name          = "${local.fullname}-gateway_api"
  protocol_type = "HTTP"

  cors_configuration {
	allow_origins = ["*"]
	allow_methods = ["POST", "OPTIONS"]
	allow_headers = ["content-type", "authorization"]
	max_age = 300
  }

  # change this to true when you create a cloudfront distribution for the api
  disable_execute_api_endpoint = false
}

resource "aws_apigatewayv2_stage" "gateway_stage" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  name        = "$default"
  auto_deploy = true

  access_log_settings {
	destination_arn = aws_cloudwatch_log_group.apigw_log_group.arn

	format = jsonencode({
	  requestId               = "$context.requestId"
	  sourceIp                = "$context.identity.sourceIp"
	  requestTime             = "$context.requestTime"
	  protocol                = "$context.protocol"
	  httpMethod              = "$context.httpMethod"
	  resourcePath            = "$context.resourcePath"
	  routeKey                = "$context.routeKey"
	  status                  = "$context.status"
	  responseLength          = "$context.responseLength"
	  integrationErrorMessage = "$context.integrationErrorMessage"
	})
  }
}

resource "aws_apigatewayv2_integration" "apigw_integration" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  integration_uri    = aws_lambda_function.lambda_function.invoke_arn
  integration_type   = "AWS_PROXY"
  integration_method = "POST"
  payload_format_version = "2.0"
}

resource "aws_apigatewayv2_route" "apigw_route" {
  api_id = aws_apigatewayv2_api.gateway_api.id

  route_key = "POST /"
  target    = "integrations/${aws_apigatewayv2_integration.apigw_integration.id}"
}

resource "aws_cloudwatch_log_group" "apigw_log_group" {
  name = "/aws/api_gw/${aws_apigatewayv2_api.gateway_api.name}"
  retention_in_days = 90
}

resource "aws_lambda_permission" "apigw_permission" {
  statement_id  = "AllowExecutionFromAPIGateway"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.lambda_function.function_name
  principal     = "apigateway.amazonaws.com"

  source_arn = "${aws_apigatewayv2_api.gateway_api.execution_arn}/*/*"
}

# # domain mapping to enable a domain name
# variable "domain_acm_cert_arn" { type = string }
# resource "aws_apigatewayv2_domain_name" "apigw_domain" {
#   domain_name = "api.${local.domain_name}"
#   domain_name_configuration {
#     certificate_arn = var.domain_acm_cert_arn
#     endpoint_type   = "REGIONAL"
#     security_policy = "TLS_1_2"
#   }
# }

# resource "aws_apigatewayv2_api_mapping" "apigw_domain_mapping" {
#   api_id      = aws_apigatewayv2_api.gateway_api.id
#   domain_name = aws_apigatewayv2_domain_name.apigw_domain.id
#   stage  = aws_apigatewayv2_stage.gateway_stage.id
# }

# output "api_url" { value = "https://${aws_apigatewayv2_domain_name.apigw_domain.id}/" }
output "api_url" { value = "${aws_apigatewayv2_stage.gateway_stage.invoke_url}" }
''')
api_lambda_test = Definition("scripts/test.sh", append=True, text='''
INVOKE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{lambda_name}}_api_url.value')
echo "invoking api: $INVOKE_URL"
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -d '{"action":"/{{lambda_name}}/hello","msg":"working as intended"}'
''')

graphql_lambda_main = Definition("{{lambda_name}}/main.py", text='''import sys
sys.path.append('modules')
import os
import json
import boto3
import traceback

from schema import schema


# aws api clients
cloudwatch_client = boto3.client('cloudwatch')
# env variables passed in from infrastructure
metrics_group = os.environ.get('METRICS_GROUP')

def internal_lambda_handler(event, context):
	result = schema.execute(json.loads(event['body'])['query'], context={ 'authorization': event['headers'].get('authorization') })
	# print('[r] graphql result:', result.data)
	return result.formatted

def lambda_handler(event, context):
	print('[i] handling event:', event)
	try:
		# perform routing
		res = internal_lambda_handler(event, context)
		print('[r] response:', res)
		if res.get('errors') is None:
			send_metric('lambda-success')
		else:
			send_metric('lambda-error')
		return res
	except Exception as e:
		print('[!] exception thrown:', e, traceback.format_exception(None, e, e.__traceback__))
		# track crash metrics
		send_metric('lambda-crash')
		res = { 'data': None, 'errors': 'server-side error' }
		print('[r] response:', res)
		return res

def send_metric(name):
	cloudwatch_client.put_metric_data(Namespace=metrics_group,
		MetricData = [{
			'MetricName': name,
			'Unit': 'Count',
			'Value': 1
		}])
''')

graphql_lambda_authorization = Definition("{{lambda_name}}/authorization.py", text='''import sys
sys.path.append('modules')
import os
import json
import traceback
import hashlib
import hmac
import datetime

import jwt

api_authorization_secret_key = os.environ.get('API_AUTHORIZATION_SECRET_KEY')
api_domain_name = os.environ.get('API_DOMAIN_NAME')



def sign_jwt_authorization(email, role):
	print('[i] request to sign jwt token from:', email, 'for:', api_domain_name)
	jwt_token = jwt.encode({
			'email': email,
			'role': role,
			'iss': api_domain_name,
			'aud': api_domain_name,
			'exp': datetime.datetime.now(tz=datetime.timezone.utc) + datetime.timedelta(seconds=60 * 60 * 24),
		}, api_authorization_secret_key, algorithm='HS256')
	print('[+] signed jwt token:', jwt_token)

	return jwt_token

def verify_jwt_authorization(token):
	print('[i] got jwt token:', token)
	try:
		jwt_payload = jwt.decode(
				token,
				api_authorization_secret_key,
				issuer=api_domain_name,
				audience=api_domain_name,
				algorithms=["HS256"])
		print('[+] got jwt payload:', jwt_payload)

	except Exception as e:
		print('[!] exception thrown during jwt decoding:', e, traceback.format_exception(None, e, e.__traceback__))
		return None

	return {
		'email': jwt_payload['email'],
		'role': jwt_payload['role']
	}

def hash_new_password(password):
	salt = os.urandom(16).hex()
	pw_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 1000).hex()
	return salt + '/' + pw_hash

def verify_password(password, previous):
	salt, old_hash = previous.split('/', 1)
	pw_hash = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 1000).hex()
	return pw_hash == old_hash

def authorized(authorized_role):
	def inner_authorized(f):
		def wrapper(root, info, *args, **kwargs):
			account = verify_jwt_authorization(info.context.get('authorization'))
			if account is None:
				raise Exception('unauthorized')
			elif account['role'] is None or account['role'] != authorized_role:
				raise Exception('unauthorized')
			else:
				info.context['parsed_authorization'] = account
				return f(root, info, *args, **kwargs)
		return wrapper
	return inner_authorized
''')

graphql_lambda_schema = Definition("{{lambda_name}}/schema.py", text='''import sys
sys.path.append('modules')

from graphene import ObjectType, String, Schema
from authorization import authorized, sign_jwt_authorization

class Query(ObjectType):
	hello = String(name=String(default_value="stranger"))
	login = String(email=String(), password=String())
	verify = String()

	def resolve_hello(root, info, name):
		return f'Hello {name}!'
	def resolve_login(root, info, email, password):
		return sign_jwt_authorization(email, 'user')
	@authorized('user')
	def resolve_verify(root, info):
		return 'verified'

schema = Schema(query=Query)
''')

graphql_lambda_requirements = Definition("{{lambda_name}}/requirements.txt", text='''graphene
pyjwt
''')
graphql_lambda_test = Definition("scripts/test.sh", append=True, text='''
INVOKE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{lambda_name}}_api_url.value')
echo "invoking api: $INVOKE_URL"
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -d '{"query":"{ hello login(email:\"asdf\", password:\"asdf\") }"}'
''')

usermanager_lambda_schema = Definition("{{lambda_name}}/schema.py", append=True, text='''
from authorization import authorized, sign_jwt_authorization, hash_new_password, verify_password
from models import UserModel

class Query(ObjectType):
	login = String({{hash_key}}=String(), password=String())
	add_user = String({{hash_key}}=String(), password=String())
	verify = String()

	def resolve_login(root, info, {{hash_key}}, password):
		try:
			user = UserModel.get({{hash_key}})
			print('[+] found user:', user)
			if verify_password(password, user.password_hash):
				print('[+] logging in user with role:', user.role)
				return sign_jwt_authorization({{hash_key}}, user.role)
			else:
				print('[!] incorrect password, rejecting')
				return None
		except UserModel.DoesNotExist:
			print('[-] user not found:', {{hash_key}})
			return None
	def resolve_add_user(root, info, {{hash_key}}, password):
		try:
			user = UserModel.get({{hash_key}})
			print('[!] user already exists:', user)
			return None
		except UserModel.DoesNotExist:
			password_hash = hash_new_password(password)
			print('[i] adding user:', {{hash_key}})
			user = UserModel({{hash_key}}, password_hash=password_hash, role='customer')
			user.save()
			print('[+] user added:', user)
			return 'ok'
	@authorized('customer')
	def resolve_verify(root, info):
		return 'verified'

schema = Schema(query=Query)''')
usermanager_lambda_models = Definition("{{lambda_name}}/models.txt", append=True, text='''import os
from pynamodb.models import Model
from pynamodb.attributes import UnicodeAttribute

class UserModel(Model):
	class Meta:
		table_name = os.environ.get('USERS_TABLE')
	{{hash_key}} = UnicodeAttribute(hash_key=True)
	password_hash = UnicodeAttribute()
	role = UnicodeAttribute()
''')
usermanager_lambda_requirements = Definition("{{lambda_name}}/requirements.txt", append=True, text='''graphene
pynamodb
''')
usermanager_lambda_test = Definition("scripts/test.sh", append=True, text='''
INVOKE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.usermanager_lambda_api_url.value')
echo "invoking api: $INVOKE_URL"
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ addUser(email:\"qwer\", password:\"asdf\") }"}'
# curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ login(email:\"qwer\", password:\"asdf\") }"}'
# token="YOUR_TOKEN_HERE"
# curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ verify }"}'
''')


crud_lambda_authorization = Definition("{{lambda_name}}/authorization.py", append_if_not_exist=True, ignore_missing=True, text='''
def with_model(object_model):
	def inner_with_object(f):
		def wrapper(root, info, id, *args, **kwargs):
			model = get_model_by_class(object_model, id, info.context['parsed_authorization']['email'])
			if model is None:
				raise Exception('not found')
			else:
				return f(root, info, model, *args, **kwargs)
		return wrapper
	return inner_with_object
''')

crud_lambda_models = Definition("{{lambda_name}}/models.py", append=True, ignore_missing=True, text='''import os
from pynamodb.models import Model
from pynamodb.attributes import UnicodeAttribute

class {{table_name}}Model(Model):
	class Meta:
		table_name = os.environ.get('{{table_name}}_TABLE')
	id = UnicodeAttribute(hash_key=True)
	owner_id = UnicodeAttribute()
	content = UnicodeAttribute()
''')

crud_lambda_schema = Definition("{{lambda_name}}/schema.py", append=True, text='''

from graphene import ObjectType, String, Schema, List
from authorization import authorized, with_model, sign_jwt_authorization, hash_new_password, verify_password
from models import {{table_name}}Model

	add_{{table_name}} = String(content=String())
	list_{{table_name}}s = List(String)
	get_{{table_name}} = String(id=String())
	update_{{table_name}} = String(id=String(), content=String())
	delete_{{table_name}} = String(id=String())

	@authorized('customer')
	def resolve_add_{{table_name}}(root, info, content):
		new_id = str(uuid.uuid4())
		print('[i] adding {{table_name}}:', new_id, 'for user:', info.context['parsed_authorization']['email'])
		dashboard = {{table_name}}Model(new_id, owner_id=info.context['parsed_authorization']['email'], content=content)
		dashboard.save()
		print('[+] {{table_name}} added:', dashboard)
		return new_id
	@authorized('customer')
	def resolve_list_{{table_name}}s(root, info):
		results = []
		print("[i] querying dashboards for user:", info.context['parsed_authorization']['email'])
		for dashboard in {{table_name}}Model.scan({{table_name}}Model.owner_id == info.context['parsed_authorization']['email']):
			results.append(dashboard.id)
		print("[+] got dashboards:", results)
		return results
	@authorized('customer')
	@with_model({{table_name}}Model)
	def resolve_get_{{table_name}}(root, info, dashboard):
		return dashboard.content
	@authorized('customer')
	@with_model({{table_name}}Model)
	def resolve_update_{{table_name}}(root, info, dashboard, content):
		print("[i] updating dashboard:", dashboard.id)
		dashboard.content = content
		dashboard.save()
		return 'ok'
	@authorized('customer')
	@with_model({{table_name}}Model)
	def resolve_delete_{{table_name}}(root, info, dashboard):
		print("[i] deleting dashboard:", dashboard.id)
		dashboard.delete()
		return 'ok'

''')
crud_lambda_test = Definition("scripts/test.sh", append=True, text='''
INVOKE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.usermanager_lambda_api_url.value')
echo "invoking api: $INVOKE_URL"
token=$(curl -s -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ login(email:\"asdf\", password:\"asdf\") }"}' | jq -r '.data.login')
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ addUserDashboard(content:\"hello\") }"}'
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ listUserDashboards }"}'
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ getUserDashboard(id:\"13254f58-fa90-406b-bbf9-e1b6291efee1\") }"}'
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ updateUserDashboard(id:\"13254f58-fa90-406b-bbf9-e1b6291efee1\", content:\"zxcvzxcv\") }"}'
curl -X POST "$INVOKE_URL" -H 'content-type: application/json' -H "authorization: $token" -d '{"query":"{ deleteUserDashboard(id:\"13254f58-fa90-406b-bbf9-e1b6291efee1\") }"}'
''')


website_s3_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{website_name}}_build_path" { default = "../{{website_name}}/dist" }

module "{{website_name}}" {
  source = "./{{website_name}}"

  domain_name = "{{domain_name}}"
  build_directory = var.{{website_name}}_build_path
}

output "{{website_name}}_endpoint" { value = "${module.{{website_name}}.website_endpoint}" }
''')

website_s3_definition = Definition("infrastructure/{{website_name}}/{{website_name}}.tf", text='''
variable "domain_name" { type = string }
variable "build_directory" { type = string }

locals {
  content_types = {
	css  = "text/css"
	html = "text/html"
	js   = "application/javascript"
	json = "application/json"
	txt  = "text/plain"
  }
}

resource "aws_s3_bucket" "bucket" {
  bucket = var.domain_name
  force_destroy = true
}

resource "aws_s3_bucket_public_access_block" "bucket_access_block" {
  bucket = aws_s3_bucket.bucket.id

  block_public_acls       = false
  block_public_policy     = false
  ignore_public_acls      = false
  restrict_public_buckets = false
}

resource "aws_s3_bucket_policy" "bucket_policy" {
  bucket = aws_s3_bucket.bucket.id
  policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  Effect = "Allow"
	  Principal = "*"
	  Action = "s3:GetObject"
	  Resource = "arn:aws:s3:::${var.domain_name}/*"
	}]
  })
}

resource "aws_s3_bucket_website_configuration" "bucket_website_configuration" {
  bucket = aws_s3_bucket.bucket.id

  index_document {
	suffix = "index.html"
  }
  error_document {
	key = "index.html"
  }
}

resource "aws_s3_object" "build_file" {
  for_each = fileset(var.build_directory, "**")

  bucket = aws_s3_bucket.bucket.id
  key    = each.value
  source = "${var.build_directory}/${each.value}"

  content_type = lookup(local.content_types, element(split(".", each.value), length(split(".", each.value)) - 1), "text/plain")
  etag   = filemd5("${var.build_directory}/${each.value}")
}

output "website_endpoint" { value = "${aws_s3_bucket_website_configuration.bucket_website_configuration.website_endpoint}" }



# # cf distribution fronting the website, enable when useful
# variable "domain_acm_cert_arn" { type = string }
# resource "aws_cloudfront_distribution" "s3_distribution" {
#   origin {
#     domain_name              = aws_s3_bucket_website_configuration.bucket_website_configuration.website_endpoint
#     # domain_name              = aws_s3_bucket.bucket.bucket_regional_domain_name
#     # origin_access_control_id = aws_cloudfront_origin_access_control.default.id
#     origin_id                = "website_origin"
#     custom_origin_config {
#       http_port                = 80
#       https_port               = 443
#       origin_keepalive_timeout = 5
#       origin_protocol_policy   = "http-only"
#       origin_read_timeout      = 30
#       origin_ssl_protocols     = [
#         "TLSv1.2",
#       ]
#     }
#   }

#   aliases = [ var.domain_name ]

#   enabled             = true
#   is_ipv6_enabled     = true
#   default_root_object = "index.html"

#   default_cache_behavior {
#     allowed_methods  = ["DELETE", "GET", "HEAD", "OPTIONS", "PATCH", "POST", "PUT"]
#     cached_methods   = ["GET", "HEAD"]
#     target_origin_id = "website_origin"

#     forwarded_values {
#       query_string = false

#       cookies {
#         forward = "none"
#       }
#     }

#     viewer_protocol_policy = "allow-all"
#     min_ttl                = 0
#     default_ttl            = 3600
#     max_ttl                = 86400
#   }

#   restrictions {
#     geo_restriction {
#       restriction_type = "none"
#       locations        = []
#     }
#   }

#   price_class = "PriceClass_100"

#   viewer_certificate {
#     acm_certificate_arn = var.domain_acm_cert_arn
#     ssl_support_method = "sni-only"
#   }
# }

# output "distribution_id" { value = "${aws_cloudfront_distribution.s3_distribution.id}" }
''')
website_s3_dockerfile = Definition("{{website_name}}/docker/Dockerfile", text='''#!/usr/bin/env -S bash -c "docker run -v \${PWD}:/app -p3000:3000 --rm -it \$(docker build -q docker) \$@"
FROM ubuntu:20.04
WORKDIR /app

ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update \\
	&& apt-get install -y curl nano jq zip

RUN curl -sL https://deb.nodesource.com/setup_14.x | bash -

# install postgresql
ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update \\
	&& apt-get install -y awscli \\
	&& apt-get install -y openssh-client \\
	&& apt-get install -y nodejs \\
	&& apt autoclean \\
	&& apt autoremove \\
	&& apt clean

RUN useradd -ms /bin/bash runuser
USER runuser

CMD npm start
''')
website_s3_package_json = Definition("{{website_name}}/package.json", text='''{
  "name": "react-site",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "scripts": {
	"start": "webpack serve",
	"build": "rm -rf dist && webpack . --mode production && cp -rf public/* dist && rm dist/**.map",
	"test": "test"
  },
  "author": "",
  "devDependencies": {
	"@babel/cli": "^7.19.3",
	"@babel/core": "^7.20.5",
	"@babel/eslint-parser": "^7.19.1",
	"@babel/plugin-transform-runtime": "^7.19.6",
	"@babel/preset-env": "^7.20.2",
	"@babel/preset-react": "^7.18.6",
	"@babel/runtime": "^7.20.6",
	"babel-loader": "^9.1.0",
	"html-webpack-plugin": "^5.5.0",
	"webpack": "^5.75.0",
	"webpack-cli": "^5.0.0",
	"webpack-dev-server": "^4.11.1"
  },
  "dependencies": {
	"react": "^18.2.0",
	"react-dom": "^18.2.0",
	"react-router-dom": "^6.4.4"
  }
}''')
website_s3_webpack_config = Definition("{{website_name}}/webpack.config.js", text='''const path = require("path");
const HtmlWebpackPlugin = require("html-webpack-plugin");

/*We are basically telling webpack to take index.js from entry. Then check for all file extensions in resolve. 
After that apply all the rules in module.rules and produce the output and place it in main.js in the public folder.*/

module.exports={
	/** "mode"
	 * the environment - development, production, none. tells webpack 
	 * to use its built-in optimizations accordingly. default is production 
	 */
	mode: "development", 
	/** "entry"
	 * the entry point 
	 */
	entry: "./index.js", 
	devtool: 'source-map',
	output: {
		filename: "bundle.[hash].js",
		path: path.resolve(__dirname, "dist"),
	},
	/** "target"
	 * setting "node" as target app (server side), and setting it as "web" is 
	 * for browser (client side). Default is "web"
	 */
	target: "web",
	devServer: {
		/** "port" 
		 * port of dev server
		*/
		port: "3000",
		/** "static" 
		 * This property tells Webpack what static file it should serve
		*/
		static: ["./public"],
		/** "open" 
		 * opens the browser after server is successfully started
		*/
		open: true,
		/** "hot"
		 * enabling and disabling HMR. takes "true", "false" and "only". 
		 * "only" is used if enable Hot Module Replacement without page 
		 * refresh as a fallback in case of build failures
		 */
		hot: false ,
		/** "liveReload"
		 * disable live reload on the browser. "hot" must be set to false for this to work
		*/
		liveReload: true,
		historyApiFallback: true,
	},
	plugins: [
		new HtmlWebpackPlugin({
		  template: "./src/index.html",
		}),
	],
	resolve: {
		/** "extensions" 
		 * If multiple files share the same name but have different extensions, webpack will 
		 * resolve the one with the extension listed first in the array and skip the rest. 
		 * This is what enables users to leave off the extension when importing
		 */
		modules: [__dirname, "src", "node_modules"],
		extensions: ['.js','.jsx','.json', ".tsx", ".ts"] 
	},
	module:{
		/** "rules"
		 * This says - "Hey webpack compiler, when you come across a path that resolves to a '.js or .jsx' 
		 * file inside of a require()/import statement, use the babel-loader to transform it before you 
		 * add it to the bundle. And in this process, kindly make sure to exclude node_modules folder from 
		 * being searched"
		 */
		rules: [
			{
				test: /\.(js|jsx)$/,    //kind of file extension this rule should look for and apply in test
				exclude: /node_modules/, //folder to be excluded
				use:  'babel-loader' //loader which we are going to use
			}
		]
	}
}''')
website_s3_babelrc = Definition("{{website_name}}/.babelrc", text='''{
	/*
		a preset is a set of plugins used to support particular language features.
		The two presets Babel uses by default: es2015, react
	*/
	"presets": [
		"@babel/preset-env", //compiling ES2015+ syntax
		"@babel/preset-react" //for react
	],
	/*
		Babel's code transformations are enabled by applying plugins (or presets) to your configuration file.
	*/
	"plugins": [
		"@babel/plugin-transform-runtime"
	]
}''')
website_s3_index_js = Definition("{{website_name}}/index.js", text='''import React from "react";
import reactDom from "react-dom";
import { createRoot } from 'react-dom/client';
import { BrowserRouter } from "react-router-dom";
import App from "./src/App"

const root = createRoot(document.getElementById('root'));
root.render(<BrowserRouter>
		<App />
	</BrowserRouter>);
''')
website_s3_index_html = Definition("{{website_name}}/src/index.html", text='''<html lang="en">
	<head>
		<meta charset="UTF-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>{{website_name}}</title>
		<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/styles.css">
	</head>
	<body>
		<div id="root"></div>
	</body>
</html>''')
website_s3_app_js = Definition("{{website_name}}/src/App.js", text='''import React from "react";
import { Routes, Route, Navigate } from "react-router-dom";
import Home from "./pages/Home";

const App = () => {
	return (
		<div className="app">
			<Routes>
				<Route path="/" element={ <Home/> } />
			</Routes>
		</div>
	)
}

export default App
''')
website_s3_home_js = Definition("{{website_name}}/src/pages/Home.jsx", text='''import React from "react";

const Home = () => {
	return (
		<div className="homepage">
			hello world
		</div>
	)
}
export default Home
''')
website_s3_styles_css = Definition("{{website_name}}/public/css/styles.css", text='''''')
website_s3_test = Definition("scripts/test.sh", append=True, text='''
WEBSITE_URL=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{website_name}}_endpoint.value')
echo "website endpoint: $WEBSITE_URL"
curl "$WEBSITE_URL"
''')



ec2_server_module = Definition("infrastructure/main.tf", append=True, text='''
variable "{{server_name}}_build_path" { default = "../build/{{server_name}}.zip" }
variable "{{server_name}}_ssh_keypath" { default = "../.keys/{{server_name}}_key" }

module "{{server_name}}" {
  source = "./{{server_name}}"

  metrics_path = local.metrics_path
  infragroup_fullname = local.infragroup_fullname
  package_build_path = var.{{server_name}}_build_path
  ssh_keypath = var.{{server_name}}_ssh_keypath
  server_config = {}
}

output "{{server_name}}_server_ip" { value = module.{{server_name}}.server_ip }
output "{{server_name}}_instance_id" { value = module.{{server_name}}.instance_id }
''')
ec2_server_makefile = Definition("./Makefile", append=True, text='''
build_{{server_name}}_package:
	-mkdir build
	zip build/{{server_name}}.zip -FSr {{server_name}}/
	./scripts/create-keys.sh "{{server_name}}_key"
''')
ec2_server_definition = Definition("infrastructure/{{server_name}}/{{server_name}}.tf", text='''
variable "infragroup_fullname" { type = string }
variable "metrics_path" { type = string }

variable "package_build_path" { type = string }
variable "ssh_keypath" { type = string }
variable "ingress_ports" { default = [22] }
variable "server_config" { default = {} }

locals {
  fullname = "${var.infragroup_fullname}-{{server_name}}"
  metrics_group = "${var.metrics_path}/{{server_name}}"
}

resource "aws_instance" "instance" {
  ami           = "ami-0e14491966b97e8bc"
  instance_type = "t2.medium"

  security_groups = [aws_security_group.instance_security_group.name]
  key_name = aws_key_pair.instance_aws_key.id
  iam_instance_profile = aws_iam_instance_profile.instance_profile.id

  root_block_device {
	volume_size = 20
  }

  user_data = <<-EOT
	#cloud-config
	runcmd:
	  - sleep 5
	  - sudo env DEBIAN_FRONTEND=noninteractive apt -y update
	  - sudo env DEBIAN_FRONTEND=noninteractive apt install -y unzip curl awscli
	  - sudo mkdir -p /app/log
	  - sudo mkdir -p /app/config
	  - sudo chown -R ubuntu:ubuntu /app
	  - echo '${jsonencode(var.server_config)}' > /app/config/server_config.json
	  - echo 's3://${aws_s3_bucket.deploy_bucket.id}/'
	  - aws s3 cp s3://${aws_s3_bucket.deploy_bucket.id}/package.zip /app/package.zip
	  - unzip /app/package.zip -d /app
	  - sudo chown -R ubuntu:ubuntu /app
	  - cd /app/{{server_name}} && chmod +x *.sh && ./install.sh 2>&1 >> /app/log/init.log
	EOT
}

resource "aws_key_pair" "instance_aws_key" {
  key_name_prefix = "${local.fullname}-aws_key-"
  public_key = file("${var.ssh_keypath}.pub")
}

resource "aws_security_group" "instance_security_group" {
  name        = "Allow web traffic"
  description = "Allow ssh and standard http/https ports inbound and everything outbound"

  dynamic "ingress" {
	for_each = var.ingress_ports
	iterator = port
	content {
	  from_port   = port.value
	  to_port     = port.value
	  protocol    = "TCP"
	  cidr_blocks = ["0.0.0.0/0"]
	}
  }

  egress {
	from_port   = 0
	to_port     = 0
	protocol    = "-1"
	cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_iam_instance_profile" "instance_profile" {
  name = "${local.fullname}-instance_profile"
  role = aws_iam_role.instance_role.name
}

resource "aws_iam_role" "instance_role" {
  name = "${local.fullname}-instance_role"
  path = "/"

  assume_role_policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  Action = "sts:AssumeRole"
	  Effect = "Allow"
	  Principal = {
		Service = "ec2.amazonaws.com"
	  }
	}]
  })
}

resource "aws_iam_policy" "instance_policy" {
  name = "${local.fullname}-instance_policy"

  policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
	{
	  "Action": [
		"s3:Get*",
		"s3:List*"
	  ],
	  "Effect": "Allow",
	  "Resource": [
		"arn:aws:s3:::${aws_s3_bucket.deploy_bucket.id}",
		"arn:aws:s3:::${aws_s3_bucket.deploy_bucket.id}/*"
	  ]
	}
  ]
}
EOF
}

resource "aws_iam_policy_attachment" "instance_policy_attachment" {
  name       = "${local.fullname}-instance_policy_attachment"
  roles      = [aws_iam_role.instance_role.name]
  policy_arn = aws_iam_policy.instance_policy.arn
}


resource "random_id" "deploy_bucket_random_id" { byte_length = 24 }
resource "aws_s3_bucket" "deploy_bucket" {
  bucket = "deploypackage-${random_id.deploy_bucket_random_id.hex}"
  force_destroy = true
}

resource "aws_s3_object" "build_file" {
  bucket = aws_s3_bucket.deploy_bucket.id
  key = "package.zip"
  source = var.package_build_path
  etag = filemd5(var.package_build_path)
}


output "server_ip" { value = "${aws_instance.instance.public_ip}" }
output "instance_id" { value = "${aws_instance.instance.id}" }
''')
ec2_server_install_sh = Definition("{{server_name}}/install.sh", text='''#!/bin/sh
echo "[+] chmoding everything"
cd /app/package
chmod +x ./*.sh
echo "[+] installing reboot script"
sudo ln -s /app/package/reboot.sh /var/lib/cloud/scripts/per-boot/reboot-init.sh
echo "[+] running main server"
./server-startup.sh
''')




dynamodb_tables_header_module = Definition("infrastructure/main.tf", append_if_not_exist=True, text='''
module "database_tables" {
  source = "./database_tables"

  infragroup_fullname = local.infragroup_fullname
}
''')

dynamodb_tables_header_definition = Definition("infrastructure/database_tables/database_tables.tf", append_if_not_exist=True, text='''
variable "infragroup_fullname" { type = string }

locals {
  fullname = "${var.infragroup_fullname}"
}
''')

dynamodb_table_module = Definition("infrastructure/main.tf", append_if_not_exist=True, text='''
output "{{table_name}}_table_id" { value = "${module.database_tables.{{table_name}}_table_id}" }
''')

dynamodb_table_definition = Definition("infrastructure/database_tables/database_tables.tf", append=True, ignore_missing=True, text='''


resource "random_id" "{{table_name}}_table_random_id" { byte_length = 8 }
resource "aws_dynamodb_table" "{{table_name}}_table" {
  name             = "${local.fullname}-{{table_name}}_table-${random_id.{{table_name}}_table_random_id.hex}"
  billing_mode     = "PAY_PER_REQUEST"
  stream_enabled   = false

  hash_key         = "{{hash_key}}"
  attribute {
	name = "{{hash_key}}"
	type = "S"
  }
}

output "{{table_name}}_table_id" { value = "${aws_dynamodb_table.{{table_name}}_table.id}" }
output "{{table_name}}_widget_json" {
  value = <<EOF
  {
	"type": "metric",
	"width": 6,
	"height": 6,
	"properties": {
	  "metrics": [
		[ "AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", "${aws_dynamodb_table.{{table_name}}_table.id}", { "label": "{{table_name}} Consumed Read" } ],
		[ ".", "ConsumedWriteCapacityUnits", ".", ".", { "label": "{{table_name}} Consumed Write" } ],
	  ],
	  "view": "timeSeries",
	  "stacked": false,
	  "region": "us-east-1",
	  "period": 300,
	  "stat": "Average",
	  "title": "{{table_name}} ddb usage"
	}
  }
EOF
}
''')

dynamodb_table_test = Definition("scripts/test.sh", append=True, text='''
TABLE_ID=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{table_name}}_table_id.value')
echo "table id: $TABLE_ID"

# insert/update an item
aws dynamodb update-item --table-name "$TABLE_ID" --region us-east-1 \\
	--key '{"{{hash_key}}": {"S": "mykey"} }' \\
	--update-expression 'SET #var = :var' \\
	--expression-attribute-names '{ "#var": "myvar" }' \\
	--expression-attribute-values '{ ":var": {"S": "myvalue"} }'
# get a single item
aws dynamodb get-item --table-name "$TABLE_ID" --region us-east-1 --key '{"{{hash_key}}": {"S": "mykey"} }' --attributes-to-get '["{{hash_key}}","myvar"]'
# list the table
aws dynamodb scan --table-name "$TABLE_ID" --region us-east-1

''')



firehose_s3_module = Definition("infrastructure/main.tf", append=True, text='''
module "{{firehose_name}}" {
  source = "./{{firehose_name}}"
  infragroup_fullname = local.infragroup_fullname
}
output "{{firehose_name}}_stream_name" { value = module.{{firehose_name}}.firehose_stream_name }
output "{{firehose_name}}_bucket_id" { value = module.{{firehose_name}}.bucket_id }

''')

firehose_s3_definition = Definition("infrastructure/{{firehose_name}}/{{firehose_name}}.tf", text='''
variable "infragroup_fullname" { type = string }

locals {
  fullname = "${var.infragroup_fullname}-{{firehose_name}}"
}

resource "aws_kinesis_firehose_delivery_stream" "extended_s3_stream" {
  name        = "${local.fullname}"
  destination = "extended_s3"

  extended_s3_configuration {
	role_arn   = aws_iam_role.firehose_role.arn
	bucket_arn = aws_s3_bucket.data_bucket.arn
  }
}

resource "random_id" "data_bucket_random_id" { byte_length = 8 }
resource "aws_s3_bucket" "data_bucket" {
  bucket = "${local.fullname}-databkt-${random_id.data_bucket_random_id.hex}"
}

resource "aws_iam_role" "firehose_role" {
  name               = "${local.fullname}-role"
  assume_role_policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  Action = "sts:AssumeRole"
	  Effect = "Allow"
	  Principal = {
		Service = "firehose.amazonaws.com"
	  }
	}]
  })
}

resource "aws_iam_policy" "firehose_policy" {
  name        = "${local.fullname}-policy"
  path        = "/"

  policy = jsonencode({
	Version = "2012-10-17"
	Statement = [{
	  "Effect" = "Allow",
	  "Action" = [
		"logs:PutLogEvents"
	  ],
	  "Resource" = [
		"arn:aws:logs:us-east-1:671999808738:log-group:/aws/kinesisfirehose/${aws_kinesis_firehose_delivery_stream.extended_s3_stream.name}:log-stream:*",
	  ]
	}, {
	  "Effect" = "Allow",
	  "Action" = [
		"s3:AbortMultipartUpload",
		"s3:GetBucketLocation",
		"s3:GetObject",
		"s3:ListBucket",
		"s3:ListBucketMultipartUploads",
		"s3:PutObject"
	  ],
	  "Resource" = [
		"arn:aws:s3:::${aws_s3_bucket.data_bucket.id}",
		"arn:aws:s3:::${aws_s3_bucket.data_bucket.id}/*"
	  ]
	}]
  })
}

resource "aws_cloudwatch_log_group" "firehose_log_group" {
  name = "/aws/kinesisfirehose/${aws_kinesis_firehose_delivery_stream.extended_s3_stream.name}"
  retention_in_days = 30
}

resource "aws_iam_role_policy_attachment" "lambda_policy_attachment" {
  role       = aws_iam_role.firehose_role.name
  policy_arn = aws_iam_policy.firehose_policy.arn
}

output "firehose_stream_name" { value = aws_kinesis_firehose_delivery_stream.extended_s3_stream.name }
output "bucket_id" { value = aws_s3_bucket.data_bucket.id }

# output "monitor_widget_json" {
#   value = <<EOF
#   {
#     "height": 6,
#     "width": 6,
#     "type": "metric",
#     "properties": {
#       "metrics": [
#         [ "${local.metrics_group}", "lambda-success", { "color": "#98df8a", "label": "lambda-success" } ],
#         [ ".", "lambda-error", { "color": "#ffbb78" } ],
#         [ ".", "lambda-crash", { "color": "#ff9896" } ]
#       ],
#       "view": "timeSeries",
#       "stacked": true,
#       "region": "us-east-1",
#       "stat": "Sum",
#       "period": 300,
#       "title": "{{firehose_name}} metrics"
#     }
#   }
# EOF
# }

''')

firehose_s3_test = Definition("scripts/test.sh", append=True, text='''
FIREHOSE_STREAM_NAME=$(cat infrastructure/terraform.tfstate | jq -r '.outputs.{{firehose_name}}_stream_name.value')
echo "putting event into firehose: $FIREHOSE_STREAM_NAME"
aws firehose put-record --region us-east-1 --delivery-stream-name $FIREHOSE_STREAM_NAME --record '{"Data":"SGVsbG8gd29ybGQhCg=="}'

''')

lambda_schedule_definition = Definition("infrastructure/{{lambda_name}}/{{lambda_name}}.tf", append=True, text='''
# event bridge scheduled event
resource "random_id" "event_api_key" { byte_length = 32 }
resource "aws_cloudwatch_event_rule" "lambda_recurring_event" {
  name = "${local.fullname}-recurring_event"
  schedule_expression = "rate(1 day)"
}
resource "aws_cloudwatch_event_target" "lambda_recurring_event_target" {
  rule = aws_cloudwatch_event_rule.lambda_recurring_event.name
  arn = aws_lambda_function.lambda_function.arn
  input = jsonencode({
	body = jsonencode({
	  action = "/{{lambda_name}}/hello"
	  event_api_key = random_id.event_api_key.hex
	})
  })
}
resource "aws_lambda_permission" "lambda_recurring_event_permission" {
  statement_id = "AllowExecutionFromCloudWatch"
  action = "lambda:InvokeFunction"
  function_name = aws_lambda_function.lambda_function.function_name
  principal = "events.amazonaws.com"
  source_arn = aws_cloudwatch_event_rule.lambda_recurring_event.arn
}
''')

terraform_s3_backend = Definition("infrastructure/main.tf", append_if_not_exist=True, text='''
terraform {
  backend "s3" {
	bucket = "{{state_bucket}}"
	key    = "terraform.tfstate"
	region = "us-east-1"
  }
}
''')

githubactions_deploy_workflow = Definition(".github/workflows/deploy.yml", text='''
name: Infra Deploy
on:
  push:
	branches:
	  - deploy

jobs:
  deploy_infra:
	runs-on: ubuntu-22.04
	permissions:
	  id-token: write # required to use OIDC authentication
	  contents: read
	steps:
	  - uses: actions/checkout@v3
	  - uses: aws-actions/configure-aws-credentials@v1
		name: configure aws credentials
		with:
		  role-to-assume: {{role_arn}}
		  role-duration-seconds: 900
		  aws-region: us-east-1
	  - name: build
		run: |
		  touch .env
		  make build
	  - name: deploy
		run: |
		  ./scripts/deploy.sh -autodeploy

''')






####################################################################################################
# template definitions
####################################################################################################


infra_base_template = TemplateDefinitions('{infra_name} infra', {
	'company_name': r"^[a-zA-Z_]+$",
	'infra_name': r"^[a-zA-Z_][a-zA-Z0-9_\-]+$"
}, [
	base_terraform_main,
	base_makefile,
	base_dockerfile,
], '''
base infrastructure established...

to run:
	put your `AWS_ACCESS_ID=` and `AWS_ACCESS_KEY=` into a .env file here
	do `chmod +x ./docker/Dockerfile`
	execute with `./docker/Dockerfile`

deploy with `weasel deploy`
destroy and clean with `weasel destroy clean`
''')


ec2_server_template = TemplateDefinitions('{server_name} server', {
	'server_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
}, [
	ec2_server_module,
	ec2_server_makefile,
	ec2_server_definition,
	ec2_server_install_sh,
], '''
ec2 server scaffolding established...
build with `weasel build_{server_name}_package`
your private ssh key will be located at `.keys/{server_name}_key` after the build
ssh into the server with `./scripts/ssh-into.sh {server_name}` after it is deployed
''')


website_s3_bucket_template = TemplateDefinitions('{website_name} bucket', {
	'website_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'domain_name': r"^[a-zA-Z_][a-zA-Z0-9_]+(\.[a-zA-Z0-9_]+)+$",
}, [
	website_s3_module,
	website_s3_definition,
	website_s3_dockerfile,
	website_s3_package_json,
	website_s3_webpack_config,
	website_s3_babelrc,
	website_s3_index_js,
	website_s3_index_html,
	website_s3_app_js,
	website_s3_home_js,
	website_s3_styles_css,
	website_s3_test,
], '''
website bucket scaffolding established...
build the website by doing:
	cd {website_name}
	chmod +x docker/Dockerfile
	./docker/Dockerfile npm i -dev
	./docker/Dockerfile npm run build
then deploy your infra as normal
''')


base_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	base_lambda_definition,
	base_lambda_makefile,
	base_lambda_module,
	base_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
	base_lambda_requirements,
], '''
basic lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
tail logs by using `aws logs tail /aws/lambda/MYLAMBDA_NAME --follow --region us-east-1 &`
''', '''

test your lambda by invoking:
	aws lambda invoke --region us-east-1 \
		--function-name <LAMBDA_FULL_NAME> \
		--payload $(echo '{ "body": "{\"action\":\"/<LAMBDA_NAME>/hello\"}" }' | base64 -w 0) \
		/dev/stdout

''')


graphql_lambda_template = TemplateDefinitions('{lambda_name} gql lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	base_lambda_definition,
	base_lambda_makefile,
	base_lambda_module,
	graphql_lambda_main,
	graphql_lambda_authorization,
	graphql_lambda_schema,
	base_lambda_build,
	graphql_lambda_requirements,
], '''
graphql lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
tail logs by using `aws logs tail /aws/lambda/MYLAMBDA_NAME --follow --region us-east-1 &`
''', '''

test your graphql lambda by invoking:
	aws lambda invoke --region us-east-1 \
		--function-name <LAMBDA_FULL_NAME> \
		--payload $(echo '{ "body": "{\"query\":\"{ hello }\"}" }' | base64 -w 0) \
		/dev/stdout
''')


api_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	api_lambda_definition,
	base_lambda_makefile,
	api_lambda_module,
	base_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
	base_lambda_requirements,
	api_lambda_test,
], '''
api lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
tail logs by using `aws logs tail /aws/lambda/MYLAMBDA_NAME --follow --region us-east-1 &`
test the lambda by executing `./scripts/test.sh`
''')


api_graphql_lambda_template = TemplateDefinitions('{lambda_name} gql lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	api_lambda_definition,
	base_lambda_makefile,
	api_lambda_module,
	graphql_lambda_main,
	graphql_lambda_authorization,
	graphql_lambda_schema,
	base_lambda_build,
	graphql_lambda_requirements,
	graphql_lambda_test,
], '''
api lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
tail logs by using `aws logs tail /aws/lambda/MYLAMBDA_NAME --follow --region us-east-1 &`
test the lambda by executing `./scripts/test.sh`
''')


sqs_lambda_template = TemplateDefinitions('{lambda_name} lambda', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	sqs_lambda_definition,
	base_lambda_makefile,
	sqs_lambda_module,
	sqs_lambda_main,
	base_lambda_lib,
	base_lambda_routes,
	base_lambda_build,
	base_lambda_requirements,
	sqs_lambda_test,
], '''
sqs lambda scaffolded...
build the lambda package with `weasel build_{lambda_name}`
tail logs by using `aws logs tail /aws/lambda/MYLAMBDA_NAME --follow --region us-east-1 &`
test the lambda by executing `./scripts/test.sh`
''')


dynamodb_table_template = TemplateDefinitions('{table_name} table', {
	'table_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'hash_key': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
}, [
	dynamodb_tables_header_module,
	dynamodb_tables_header_definition,
	dynamodb_table_module,
	dynamodb_table_definition,
	dynamodb_table_test,
], '''
dynamodb table scaffolded...
test the table by executing `./scripts/test.sh`
use the table with PynamoDB:
	$ pip install pynamodb
then:

import os
from pynamodb.models import Model
from pynamodb.attributes import UnicodeAttribute
class {table_name}(Model):
	class Meta:
		table_name = os.environ.get('{table_name}_TABLE')
	{hash_key} = UnicodeAttribute(hash_key=True)

''', '''
remember to add to the policy the following:
{
  "Effect" = "Allow",
  "Action" = [
	"dynamodb:GetItem",
	"dynamodb:PutItem",
	"dynamodb:UpdateItem",
	"dynamodb:DeleteItem",
	"dynamodb:Scan",
	"dynamodb:Query"
  ],
  "Resource" = [
	  "arn:aws:dynamodb:us-east-1:*:table/${var.arg_table}",
  ]
}


enjoy responsibly...
''')


usermanager_ddb_table_template = TemplateDefinitions('{lambda_name} usermanager lambda', {
	'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'table_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'hash_key': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
}, [
	dynamodb_tables_header_module,
	dynamodb_tables_header_definition,
	dynamodb_table_module,
	dynamodb_table_definition,
	dynamodb_table_test,
	usermanager_lambda_schema,
	usermanager_lambda_models,
	usermanager_lambda_requirements,
	usermanager_lambda_test,
], '''
usermanager dynamodb table scaffolded...
graphql endpoint additions added to {table_name} lambda...
test the user system by executing `./scripts/test.sh`

''', '''
remember to add to the policy the following:
{
  "Effect" = "Allow",
  "Action" = [
	"dynamodb:GetItem",
	"dynamodb:PutItem",
	"dynamodb:UpdateItem",
	"dynamodb:DeleteItem",
	"dynamodb:Scan",
	"dynamodb:Query"
  ],
  "Resource" = [
	  "arn:aws:dynamodb:us-east-1:*:table/${var.arg_table}",
  ]
}

and add the env variables to the lambda terraform:
	resource "random_id" "random_api_authentication_secret_key" { byte_length = 64 }
	...
	API_AUTHORIZATION_SECRET_KEY = random_id.random_api_authentication_secret_key.hex
	API_DOMAIN_NAME = "example.com"
	USERS_TABLE = var.users_table


enjoy responsibly...
''')


crud_ddb_table_template = TemplateDefinitions('{lambda_name} lambda', {
	'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
	'table_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$",
}, [
	dynamodb_tables_header_module,
	dynamodb_tables_header_definition,
	dynamodb_table_module,
	dynamodb_table_definition,
	dynamodb_table_test,
	crud_lambda_authorization,
	crud_lambda_models,
	crud_lambda_schema,
	crud_lambda_test,
], '''
CRUD dynamodb table scaffolded...
graphql endpoint additions added to {table_name} lambda...
test the crud apis by executing `./scripts/test.sh`

''', '''
remember to add to the policy the following:
{
  "Effect" = "Allow",
  "Action" = [
	"dynamodb:GetItem",
	"dynamodb:PutItem",
	"dynamodb:UpdateItem",
	"dynamodb:DeleteItem",
	"dynamodb:Scan",
	"dynamodb:Query"
  ],
  "Resource" = [
	  "arn:aws:dynamodb:us-east-1:*:table/${var.arg_table}"
  ]
}

enjoy responsibly...
''')


firehose_s3_template = TemplateDefinitions('{firehose_name} firehose', {
	'firehose_name': r"^[a-zA-Z][a-zA-Z0-9]+$",
}, [
	firehose_s3_module,
	firehose_s3_definition,
	firehose_s3_test,
], '''
s3-backed firehose scaffolded...
test the firehose by executing `./scripts/test.sh`
''', '''
use the firehose with Python:

import boto3
firehose_client = boto3.client('firehose', region_name='us-east-1')
firehose_client.put_record(
	DeliveryStreamName='STREAM NAME HERE!!!!',
	Record={ 'Data': b'bytes' })


remember to add the following policy:
{
  "Effect" = "Allow",
  "Action" = [
	"firehose:PutRecord",
  ],
  "Resource" = [
	  "arn:aws:firehose:us-east-1:*:deliverystream/*",
  ]
}


drink up...
''')


lambda_schedule_template = TemplateDefinitions('{lambda_name} lambda schedule event', { 'lambda_name': r"^[a-zA-Z_][a-zA-Z0-9_]+$" }, [
	lambda_schedule_definition,
], '''
lambda event bridge job schedule created on a 1-per-day...
add `EVENT_API_KEY_HASH = sha256(random_id.event_api_key.hex)` to your lambda env to verify that the invocation came from the eventbridge
''')

githubactions_deploy_template = TemplateDefinitions('github actions deploy', { 'role_arn': r"^\S+$", 'state_bucket': r"^\S{3,63}$" }, [
	terraform_s3_backend,
	githubactions_deploy_workflow,
], '''
Follow this guide for how to set up the role:
https://benoitboure.com/securely-access-your-aws-resources-from-github-actions

also make sure to create your state bucket
!!! Make sure your bucket has all access disabled !!!
your aws role should have sufficient permissions to acces the bucket
no public access is necessary...

once your role and state bucket are set up, you should just push to your `deploy` branch, and everything will deploy automatically

use `aws s3 cp s3://{state_bucket}/terraform.tfstate infrastructure/terraform.tfstate` in your test.sh to keep it working
''')


####################################################################################################
# main
####################################################################################################


# wrap to catch sigint
try:
	# parse arguments
	parser = argparse.ArgumentParser(prog='stonemill', description='A terraform scaffolding tool')
	parser.add_argument('--infra-base', nargs=2, help='creates the starter Makefile and main.tf;\t stonemill --infra-base mycompany myproject')
	parser.add_argument('--lambda-function', nargs=1, help='creates a py3 lambda;\t stonemill --lambda-function my_lambda')
	parser.add_argument('--api-lambda-function', nargs=1, help='creates a py3 lambda with an api gateway;\t stonemill --api-lambda-function my_lambda')
	parser.add_argument('--sqs-lambda-function', nargs=1, help='creates a py3 lambda with an sqs input queue;\t stonemill --sqs-lambda-function my_lambda')
	parser.add_argument('--gql-lambda-function', nargs=1, help='creates a py3 graphql lambda;\t stonemill --gql-lambda-function my_lambda')
	parser.add_argument('--api-gql-lambda-function', nargs=1, help='creates a py3 graphql lambda with an api gateway;\t stonemill --api-gql-lambda-function my_lambda')
	parser.add_argument('--website-s3-bucket', nargs=2, help='creates an s3 bucket for hosting a react site;\t stonemill --website-s3-bucket my_website myspecialfrontend.com')
	parser.add_argument('--ec2-server', nargs=1, help='creates an ec2 server with ssh key;\t stonemill --ec2-server my_server')
	parser.add_argument('--dynamodb-table', nargs=2, help='creates a basic dynamodb table;\t stonemill --dynamodb-table my_accounts account_id')
	parser.add_argument('--usermanager-ddb-table', nargs=2, help='creates a users dynamodb table with graphql endpoints for addUser and login;\n\t uses strong password salting and hashing for security;\t stonemill --usermanager-ddb-table my_lambda users email')
	parser.add_argument('--crud-ddb-table', nargs=2, help='creates a dynamodb table with graphql endpoints for Create/Read/Update/Delete/List;\n\t uses uuid4 for ids and has strong authentication on the user;\t stonemill --crud-ddb-table my_lambda my_model')
	parser.add_argument('--firehose-s3', nargs=1, help='creates a kinesis firehose that puts to an s3 bucket;\t stonemill --firehose-s3 my_firehose_name')
	parser.add_argument('--lambda-schedule-event', nargs=1, help='creates an eventbridge job that triggers lambda on a schedule;\t stonemill --lambda-schedule-event my_lambda')
	parser.add_argument('--githubactions-deploy', nargs=2, help='creates the deployment workflows for github actions to push the terraform infrastructure;\t stonemill --githubactions-deploy "arn:aws:iam::123456789012:role/GithubActionsAccessRole" "tfstate-abcdef0123456789"\n\t follow this guide for how to create your role and secure it to only allow github access: https://benoitboure.com/securely-access-your-aws-resources-from-github-actions')
	args = parser.parse_args()

	if args.infra_base:
		template_args = infra_base_template.parse_arguments(company_name=args.infra_base[0], infra_name=args.infra_base[1])
		infra_base_template.mill_template(template_args)

	elif args.ec2_server:
		template_args = ec2_server_template.parse_arguments(server_name = args.ec2_server[0])
		ec2_server_template.mill_template(template_args)

	elif args.website_s3_bucket:
		template_args = website_s3_bucket_template.parse_arguments(website_name = args.website_s3_bucket[0], domain_name = args.website_s3_bucket[1])
		website_s3_bucket_template.mill_template(template_args)

	elif args.lambda_function:
		template_args = base_lambda_template.parse_arguments(lambda_name = args.lambda_function[0])
		base_lambda_template.mill_template(template_args)

	elif args.gql_lambda_function:
		template_args = graphql_lambda_template.parse_arguments(lambda_name = args.gql_lambda_function[0])
		graphql_lambda_template.mill_template(template_args)

	elif args.api_gql_lambda_function:
		template_args = api_graphql_lambda_template.parse_arguments(lambda_name = args.api_gql_lambda_function[0])
		api_graphql_lambda_template.mill_template(template_args)

	elif args.api_lambda_function:
		template_args = api_lambda_template.parse_arguments(lambda_name = args.api_lambda_function[0])
		api_lambda_template.mill_template(template_args)

	elif args.sqs_lambda_function:
		template_args = sqs_lambda_template.parse_arguments(lambda_name = args.sqs_lambda_function[0])
		sqs_lambda_template.mill_template(template_args)

	elif args.dynamodb_table:
		template_args = dynamodb_table_template.parse_arguments(table_name = args.dynamodb_table[0], hash_key = args.dynamodb_table[1])
		dynamodb_table_template.mill_template(template_args)

	elif args.usermanager_ddb_table:
		template_args = usermanager_ddb_table_template.parse_arguments(lambda_name = args.usermanager_ddb_table[0], table_name = args.usermanager_ddb_table[1], hash_key = args.usermanager_ddb_table[2])
		usermanager_ddb_table_template.mill_template(template_args)

	elif args.crud_ddb_table:
		template_args = crud_ddb_table_template.parse_arguments(lambda_name = args.crud_ddb_table[0], table_name = args.crud_ddb_table[1])
		crud_ddb_table_template.mill_template(template_args)

	elif args.firehose_s3:
		template_args = firehose_s3_template.parse_arguments(firehose_name = args.firehose_s3[0])
		firehose_s3_template.mill_template(template_args)

	elif args.lambda_schedule_event:
		template_args = lambda_schedule_template.parse_arguments(lambda_name = args.lambda_schedule_event[0])
		lambda_schedule_template.mill_template(template_args)

	elif args.githubactions_deploy:
		template_args = githubactions_deploy_template.parse_arguments(role_arn = args.githubactions_deploy[0], state_bucket = args.githubactions_deploy[1])
		githubactions_deploy_template.mill_template(template_args)

	else:
		parser.print_help()

except KeyboardInterrupt:
	sys.exit(1)



